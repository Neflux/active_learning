{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST('', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST('', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "net = LinearNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.286768\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 0.394023\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 0.415429\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 0.356281\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.226125\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.252468\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.039202\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.029337\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 1.186294\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.023177\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.163831\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.547942\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.038102\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.123143\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.259829\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.026426\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.282001\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.006606\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.139225\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.126994\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.001662\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.225690\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.009981\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.707447\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.384083\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.100250\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.025529\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.060402\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.002102\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.555991\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.205910\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.018158\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.068854\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.001374\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.600462\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.893684\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.264938\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.041401\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.225751\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.232679\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.014888\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.005813\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.047952\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.334912\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.062945\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.297412\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.071437\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.231844\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.056765\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.027506\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.011632\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.008627\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.063480\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.002537\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.120069\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.011608\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.033206\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.122060\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.006013\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.016655\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.173540\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.001949\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.027073\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.810171\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.792211\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.009453\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.042987\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.004308\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.007314\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.005094\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.273856\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.062654\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.194394\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.000938\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.000349\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.011109\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.214038\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.009918\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.015882\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.010784\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.002241\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.004120\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.013736\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.022988\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.014906\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.011606\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.009381\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.107630\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.083876\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.004184\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(X), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9698/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        output = net(X.view(-1,28*28))\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        test_loss += F.nll_loss(output, y, reduction='sum').item()\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b3af80d55cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Classifier says: {torch.argmax(net(X[0].view(-1,28*28)))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "X, _ = next(iter(train_loader))\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()\n",
    "print(f\"Classifier says: {torch.argmax(net(X[0].view(-1,28*28)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(32*3*32*3, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "        \n",
    "net = ConvNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.296376\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.518402\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.261146\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.124958\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.204115\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.099183\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.275552\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.212750\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.242164\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.322103\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.019931\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.004057\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.427906\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 1.276977\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.653371\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.002903\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.381754\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.001100\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.001487\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.265080\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.025060\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.750718\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.040581\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.015401\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.006131\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.615925\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.018493\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.234992\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.000836\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.106590\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.002362\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.000186\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.017992\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.569498\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.131548\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.353295\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.024700\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.000145\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.010043\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.000777\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.031930\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.014237\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.518074\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.002977\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.058360\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.034925\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.015022\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.049998\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.000371\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.058432\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.138753\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.005354\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.006138\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.015633\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.007608\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.045207\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.006148\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.004100\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.037399\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.005557\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.000746\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.002661\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.073568\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.050628\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.000203\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.001946\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.000589\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.004932\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.021147\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.065877\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.002107\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.016840\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.007519\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.022251\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.001274\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.001188\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.070328\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.119716\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.007014\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.077105\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.252468\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.135908\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.054787\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.120055\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.000559\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.018503\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.077593\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.006596\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.000081\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.031117\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 1, 28,28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(X), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0808, Accuracy: 9778/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        output = net(X.view(-1,1,28,28))\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        test_loss += F.nll_loss(output, y, reduction='sum').item()\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier says: 5\n"
     ]
    }
   ],
   "source": [
    "X, _ = next(iter(train_loader))\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()\n",
    "print(f\"Classifier says: {torch.argmax(net(X[0].view(-1,1,28,28)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian fully connected net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./BayesianNeuralNetworks\")\n",
    "import bnn.nn as nn\n",
    "#from bnn.nn import BayesianNetworkModule, NormalConv2d, NormalLinear, KLDivergence, Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesConvNet(\n",
      "  (fc1): NormalLinear(\n",
      "    (weight): WeightNormal()\n",
      "    (bias): WeightNormal()\n",
      "  )\n",
      "  (fc2): NormalLinear(\n",
      "    (weight): WeightNormal()\n",
      "    (bias): WeightNormal()\n",
      "  )\n",
      "  (fc3): NormalLinear(\n",
      "    (weight): WeightNormal()\n",
      "    (bias): WeightNormal()\n",
      "  )\n",
      "  (fc4): NormalLinear(\n",
      "    (weight): WeightNormal()\n",
      "    (bias): WeightNormal()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BayesConvNet(BayesianNetworkModule):\n",
    "    def __init__(self, samples=10):\n",
    "        super(BayesConvNet, self).__init__(28*28, 10, samples)\n",
    "        \n",
    "        self.fc1 = nn.NormalLinear(28*28,64)\n",
    "        self.fc2 = nn.NormalLinear(64,64)\n",
    "        self.fc3 = nn.NormalLinear(64,64)\n",
    "        self.fc4 = nn.NormalLinear(64,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "        \n",
    "net = BayesConvNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(X), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian convolutional network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesconda",
   "language": "python",
   "name": "bayesconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
